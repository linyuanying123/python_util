import logging
import re

import mysql


def check_table_exists(cursor, table_name):
    """æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨"""
    cursor.execute("SHOW TABLES LIKE %s", (table_name,))
    return cursor.fetchone() is not None


def generate_create_table_sql(table_name, df):
    """æ ¹æ®DataFrameç”Ÿæˆåˆ›å»ºè¡¨çš„SQL"""
    columns = df.columns
    columns1 = []
    column_mapping = generate_column_mapping(columns)

    # å‡†å¤‡åˆ—åï¼Œæ›¿æ¢ç©ºæ ¼ä¸ºä¸‹åˆ’çº¿å¹¶æ˜ å°„åˆ°æ•°æ®åº“åˆ—å
    mapped_columns = [column_mapping.get(col, col) for col in columns]

    # è¿‡æ»¤æ— æ•ˆåˆ—æˆ–ä¸éœ€è¦æ’å…¥çš„åˆ—
    filtered_columns = [col for col in mapped_columns if col in column_mapping.values()]

    if not filtered_columns:
        raise ValueError("å­—æ®µåˆ—è¡¨ä¸ºç©º æˆ–è€… æ£€æŸ¥å­—æ®µæ˜¯å¦å­˜åœ¨ç‰¹æ®Šç¬¦å·ï¼Œæ— æ³•ç”Ÿæˆ CREATE TABLE è¯­å¥ã€‚")
    # è®¡ç®—åˆ—æ•°
    num_columns = len(filtered_columns)
    logging.info(f"å­—æ®µå…±æœ‰: {num_columns}ä¸ª\n")

    # MySQLè¡Œå¤§å°é™åˆ¶(65535å­—èŠ‚)ï¼Œé¢„ç•™20%ç©ºé—´ç»™è¡Œå¼€é”€å’Œä¸»é”®
    max_total_size = 65535 * 0.8  # çº¦52428å­—èŠ‚å¯ç”¨

    # è®¡ç®—æ¯åˆ—å¹³å‡å¯åˆ†é…çš„å¤§å°(utf8mb4æ¯ä¸ªå­—ç¬¦æœ€å¤šå 4å­—èŠ‚)
    avg_length = int(max_total_size / (num_columns * 4)) if num_columns > 0 else 100
    # è®¾ç½®åˆç†çš„ä¸Šä¸‹é™ (50-1000)
    field_length = max(100, min(1000, avg_length))

    logging.info(f"è®¡ç®—å‡ºçš„åŠ¨æ€å­—æ®µé•¿åº¦: {field_length} (åŸºäº{num_columns}åˆ—)")

    for col in filtered_columns:
        columns1.append(f"`{col}` VARCHAR({field_length})")
        # columns1.append(f"`{col}` VARCHAR(150)")
    columns1.append("`id` INT AUTO_INCREMENT PRIMARY KEY")
    columns_sql = ', '.join(columns1)
    return f"CREATE TABLE `{table_name}` ({columns_sql}) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_general_ci;"

def replace_pattern(s):
    return re.sub(r'\((\d+)\)', r'_\1', s)
    # return re.sub(r'\((.*?)\)', r'_\1', s)

if __name__ == '__main__':
    replace_pattern(1)

def generate_column_mapping(columns):
    """æ ¹æ®åˆ—åç”Ÿæˆæ˜ å°„è¡¨ï¼Œåˆ é™¤æ‹¬å·åŠç‰¹æ®Šå­—ç¬¦ï¼Œæ›¿æ¢ç©ºæ ¼ä¸ºä¸‹åˆ’çº¿ï¼Œå¹¶ç§»é™¤æ— æ•ˆå­—æ®µ"""
    column_mapping = {}
    for col in columns:
        handle_str = replace_pattern(col).replace('\n', '')
        cleaned_col = re.sub(r'\(.*?\)|[?ï¼Ÿ]', '', handle_str).strip()  # åˆ é™¤æ‹¬å·åŠç‰¹æ®Šç¬¦å·
        mapped_col = cleaned_col.replace('  ', '_').replace(' ', '_').replace('-', '_').replace('.', '').replace('/', '').replace('%','').replace('___','_').replace('__','_').replace(')', '')
        # if mapped_col:  # ç¡®ä¿å­—æ®µåéç©º
        #     print(col, mapped_col)
        #     column_mapping[col] = mapped_col

        # æ£€æŸ¥mapped_colæ˜¯å¦å·²ç»å­˜åœ¨äºcolumn_mappingçš„å€¼ä¸­
        counter = 90
        original_mapped_col = mapped_col
        while mapped_col in column_mapping.values():
            mapped_col = f"{original_mapped_col}_{counter}"
            counter += 1

        if mapped_col:  # ç¡®ä¿å­—æ®µåéç©º
            # print(col, mapped_col)
            column_mapping[col] = mapped_col
    return column_mapping


# if __name__ == "__main__":
#     col = "Product Family (UCS/ BBI/CloudPND (Private Network Data)/ Cloud/ China Business/ Wholesales/EMS)"
#     print(replace_pattern(col))

def import_data_to_mysql(conn, table_name, df, batch_size=1000):
    """å°†æ•°æ®æ‰¹é‡å¯¼å…¥MySQLè¡¨ä¸­ï¼Œè‡ªåŠ¨ç”Ÿæˆåˆ—åæ˜ å°„"""
    cursor = conn.cursor()

    # å°†NaNå€¼æ›¿æ¢ä¸ºç©ºå­—ç¬¦ä¸²
    df = df.fillna('')

    # æ£€æŸ¥ç¬¬ä¸€åˆ—æ˜¯å¦å­˜åœ¨ç©ºå€¼ï¼Œå¦‚æœæœ‰åˆ™æŠ›å‡ºå¼‚å¸¸
    if df.iloc[:, 0].isnull().any():
        print(" é”™è¯¯ï¼šç¬¬ä¸€åˆ—å­˜åœ¨ç©ºå€¼ï¼Œå¯¼å…¥å¤±è´¥ã€‚\n")
        return

    # è‡ªåŠ¨ç”Ÿæˆåˆ—åæ˜ å°„
    columns = df.columns
    column_mapping = generate_column_mapping(columns)

    # å‡†å¤‡åˆ—åï¼Œæ›¿æ¢ç©ºæ ¼ä¸ºä¸‹åˆ’çº¿å¹¶æ˜ å°„åˆ°æ•°æ®åº“åˆ—å
    mapped_columns = [column_mapping.get(col, col) for col in columns]

    # è¿‡æ»¤æ— æ•ˆåˆ—æˆ–ä¸éœ€è¦æ’å…¥çš„åˆ—
    filtered_columns = [col for col in mapped_columns if col in column_mapping.values()]

    # ç”Ÿæˆ SQL æ’å…¥è¯­å¥çš„å ä½ç¬¦
    placeholders = ', '.join(['%s'] * len(filtered_columns))
    # insert_sql = f"INSERT INTO `{table_name}` ({', '.join(filtered_columns)}) VALUES ({placeholders})"
    insert_sql = f"INSERT INTO `{table_name}` ({', '.join([f'`{col}`' for col in filtered_columns])}) VALUES ({placeholders})"

    print(f"ğŸ”„ å‡†å¤‡æ’å…¥æ•°æ®åˆ°è¡¨ `{table_name}`ã€‚\nç”Ÿæˆçš„ SQL:\n{insert_sql}\n")

    # å°†DataFrameè½¬æ¢ä¸ºé€‚åˆæ‰¹é‡æ’å…¥çš„æ ¼å¼
    rows_to_insert = df[columns].values.tolist()

    try:
        # æ‰¹é‡æ’å…¥æ•°æ®ï¼ŒæŒ‰æ‰¹æ¬¡åˆ†å‰²
        for i in range(0, len(rows_to_insert), batch_size):
            batch = rows_to_insert[i:i + batch_size]
            cursor.executemany(insert_sql, batch)
            conn.commit()  # æ¯æ¬¡æäº¤ä¸€ä¸ªæ‰¹æ¬¡
            print(f" å·²å¯¼å…¥ {i + len(batch)} / {len(rows_to_insert)} æ¡æ•°æ®...\n")

        print(f"ğŸ‰ æ•°æ®å·²æˆåŠŸæ‰¹é‡å¯¼å…¥åˆ°è¡¨ `{table_name}` ä¸­ï¼\n")

    except mysql.connector.Error as err:
        print(f" å¯¼å…¥æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {err}\n")
        conn.rollback()  # å¦‚æœå‡ºç°é”™è¯¯ï¼Œå›æ»šäº‹åŠ¡
    finally:
        cursor.close()



def import_data_to_mysql_V2(conn, table_name, df, batch_size=1000):
    """å°†æ•°æ®æ‰¹é‡å¯¼å…¥MySQLè¡¨ä¸­ï¼Œè‡ªåŠ¨ç”Ÿæˆåˆ—åæ˜ å°„"""
    cursor = conn.cursor()

    # å°†NaNå€¼æ›¿æ¢ä¸ºç©ºå­—ç¬¦ä¸²
    df = df.fillna('')

    # æ£€æŸ¥ç¬¬ä¸€åˆ—æ˜¯å¦å­˜åœ¨ç©ºå€¼ï¼Œå¦‚æœæœ‰åˆ™æŠ›å‡ºå¼‚å¸¸
    if df.iloc[:, 0].isnull().any():
        logging.ERROR(" é”™è¯¯ï¼šç¬¬ä¸€åˆ—å­˜åœ¨ç©ºå€¼ï¼Œå¯¼å…¥å¤±è´¥ã€‚\n")
        return

    # è‡ªåŠ¨ç”Ÿæˆåˆ—åæ˜ å°„
    columns = df.columns
    column_mapping = generate_column_mapping(columns)

    # å‡†å¤‡åˆ—åï¼Œæ›¿æ¢ç©ºæ ¼ä¸ºä¸‹åˆ’çº¿å¹¶æ˜ å°„åˆ°æ•°æ®åº“åˆ—å
    mapped_columns = [column_mapping.get(col, col) for col in columns]

    # è¿‡æ»¤æ— æ•ˆåˆ—æˆ–ä¸éœ€è¦æ’å…¥çš„åˆ—
    filtered_columns = [col for col in mapped_columns if col in column_mapping.values()]

    # ç”Ÿæˆ SQL æ’å…¥è¯­å¥çš„å ä½ç¬¦
    placeholders = ', '.join(['%s'] * len(filtered_columns))
    # insert_sql = f"INSERT INTO `{table_name}` ({', '.join(filtered_columns)}) VALUES ({placeholders})"
    insert_sql = f"INSERT INTO `{table_name}` ({', '.join([f'`{col}`' for col in filtered_columns])}) VALUES ({placeholders})"
    logging.info(f"å‡†å¤‡æ’å…¥æ•°æ®åˆ°è¡¨ `{table_name}`ã€‚")
    # å°†DataFrameè½¬æ¢ä¸ºé€‚åˆæ‰¹é‡æ’å…¥çš„æ ¼å¼
    rows_to_insert = df[columns].values.tolist()

    try:
        # æ‰¹é‡æ’å…¥æ•°æ®ï¼ŒæŒ‰æ‰¹æ¬¡åˆ†å‰²
        for i in range(0, len(rows_to_insert), batch_size):
            batch = rows_to_insert[i:i + batch_size]
            cursor.executemany(insert_sql, batch)
            conn.commit()  # æ¯æ¬¡æäº¤ä¸€ä¸ªæ‰¹æ¬¡
            logging.info(f" å·²å¯¼å…¥ {i + len(batch)} / {len(rows_to_insert)} æ¡æ•°æ®...\n")

        logging.info(f"æ•°æ®å·²æˆåŠŸæ‰¹é‡å¯¼å…¥åˆ°è¡¨ `{table_name}` ä¸­ï¼\n")

    except mysql.connector.Error as err:
        logging.error(f" å›æ»šæ’å…¥ã€‚å¯¼å…¥æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {err}\n")
        conn.rollback()  # å¦‚æœå‡ºç°é”™è¯¯ï¼Œå›æ»šäº‹åŠ¡
    finally:
        cursor.close()